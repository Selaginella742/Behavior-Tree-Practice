<?xml version="1.0" encoding="utf-8"?>
<package xmlns="http://schemas.microsoft.com/packaging/2013/05/nuspec.xsd">
  <metadata>
    <id>OllamaSharp</id>
    <version>4.0.1</version>
    <title>OllamaSharp</title>
    <authors>Andreas Wäscher, Milkey Tan, Jerrett Davis</authors>
    <license type="file">LICENSE</license>
    <licenseUrl>https://aka.ms/deprecateLicenseUrl</licenseUrl>
    <icon>Ollama.png</icon>
    <readme>README.md</readme>
    <projectUrl>https://github.com/awaescher/OllamaSharp</projectUrl>
    <description>The easiest way to use the Ollama API in .NET</description>
    <copyright>Andreas Wäscher</copyright>
    <tags>ollama ollama-api api llm openai ai localllama llama streaming gpt llamacpp ichatclient</tags>
    <repository type="git" url="https://github.com/awaescher/OllamaSharp" commit="ae92d850c860756d386b82d93f3baf12baa11197" />
    <dependencies>
      <group targetFramework=".NETStandard2.0">
        <dependency id="Microsoft.Bcl.AsyncInterfaces" version="8.0.0" exclude="Build,Analyzers" />
        <dependency id="Microsoft.Extensions.AI.Abstractions" version="9.0.0-preview.9.24525.1" exclude="Build,Analyzers" />
      </group>
    </dependencies>
  </metadata>
</package>